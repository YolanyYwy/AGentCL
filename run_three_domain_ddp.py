#!/usr/bin/env python3
"""
ä¸‰åŸŸæŒç»­å­¦ä¹ è®­ç»ƒè„šæœ¬ - åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆDDPï¼‰ç‰ˆæœ¬
ä½¿ç”¨ PyTorch DDP å®ç°çœŸæ­£çš„å¤šå¡è”è®­
"""

import argparse
import json
import sys
import os
from pathlib import Path
from datetime import datetime
from typing import List, Optional

import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import Dataset, DataLoader, DistributedSampler

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "src"))

from tau2.data_model.tasks import Task
from tau2.data_model.simulation import SimulationRun
from tau2.data_model.continual_results import TrainingMode, ContinualLearningResults, StageResult
from tau2.continual.curriculum.curriculum import Curriculum
from tau2.continual.curriculum.stage import LearningStage
from tau2.continual.evaluation.metrics import compute_continual_metrics
from tau2.run import run_task
from tau2.evaluator.evaluator import EvaluationType
from loguru import logger


def setup_ddp(rank: int, world_size: int):
    """
    åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ

    Args:
        rank: å½“å‰è¿›ç¨‹çš„ rankï¼ˆGPU ç¼–å·ï¼‰
        world_size: æ€»è¿›ç¨‹æ•°ï¼ˆGPU æ•°é‡ï¼‰
    """
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'

    # åˆå§‹åŒ–è¿›ç¨‹ç»„
    dist.init_process_group(
        backend='nccl',  # NVIDIA GPU ä½¿ç”¨ nccl
        init_method='env://',
        world_size=world_size,
        rank=rank
    )

    # è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„ GPU
    torch.cuda.set_device(rank)

    if rank == 0:
        print(f"âœ… DDP åˆå§‹åŒ–å®Œæˆ: {world_size} ä¸ª GPU")


def cleanup_ddp():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    dist.destroy_process_group()


def load_tasks_from_json(json_path: str, max_tasks: int = None) -> list[Task]:
    """ä» JSON æ–‡ä»¶åŠ è½½ä»»åŠ¡"""
    with open(json_path, 'r', encoding='utf-8') as f:
        tasks_data = json.load(f)

    if max_tasks:
        tasks_data = tasks_data[:max_tasks]

    tasks = []
    for task_dict in tasks_data:
        try:
            task = Task(**task_dict)
            tasks.append(task)
        except Exception as e:
            logger.warning(f"è·³è¿‡æ— æ•ˆä»»åŠ¡: {e}")
            continue

    return tasks


class ExperienceDataset(Dataset):
    """
    ç»éªŒå›æ”¾æ•°æ®é›†
    ç”¨äº DDP è®­ç»ƒçš„æ•°æ®é›†åŒ…è£…
    """
    def __init__(self, experiences: List[SimulationRun]):
        self.experiences = experiences

    def __len__(self):
        return len(self.experiences)

    def __getitem__(self, idx):
        return self.experiences[idx]


def collate_experiences(batch):
    """
    è‡ªå®šä¹‰ collate å‡½æ•°
    å› ä¸º SimulationRun å¯¹è±¡ä¸èƒ½ç›´æ¥ batchï¼Œæ‰€ä»¥è¿”å›åˆ—è¡¨
    """
    return batch


def run_task_single_gpu(task: Task, domain: str, agent_type: str, user_type: str,
                       model_name: str, llm_args_agent: dict, llm_args_user: dict,
                       max_steps: int, max_errors: int, seed: int, rank: int):
    """åœ¨å•ä¸ª GPU ä¸Šè¿è¡Œä»»åŠ¡"""
    try:
        run = run_task(
            domain=domain,
            task=task,
            agent=agent_type,
            user=user_type,
            llm_agent=None,
            llm_args_agent=llm_args_agent,
            llm_user=None,
            llm_args_user=llm_args_user,
            max_steps=max_steps,
            max_errors=max_errors,
            evaluation_type=EvaluationType.ALL,
            seed=seed,
        )
        return run
    except Exception as e:
        if rank == 0:
            logger.error(f"ä»»åŠ¡ {task.id} è¿è¡Œå¤±è´¥: {e}")
        return None


def gather_all_runs(runs: List[SimulationRun], rank: int, world_size: int) -> List[SimulationRun]:
    """
    æ”¶é›†æ‰€æœ‰ GPU çš„è¿è¡Œç»“æœåˆ° rank 0

    Args:
        runs: å½“å‰ GPU çš„è¿è¡Œç»“æœ
        rank: å½“å‰è¿›ç¨‹ rank
        world_size: æ€»è¿›ç¨‹æ•°

    Returns:
        æ‰€æœ‰ GPU çš„è¿è¡Œç»“æœï¼ˆä»…åœ¨ rank 0 è¿”å›å®Œæ•´åˆ—è¡¨ï¼‰
    """
    # ä½¿ç”¨ all_gather æ”¶é›†æ‰€æœ‰ GPU çš„ç»“æœ
    gathered_runs = [None] * world_size
    dist.all_gather_object(gathered_runs, runs)

    if rank == 0:
        # åœ¨ rank 0 ä¸Šåˆå¹¶æ‰€æœ‰ç»“æœ
        all_runs = []
        for gpu_runs in gathered_runs:
            if gpu_runs:
                all_runs.extend(gpu_runs)
        return all_runs
    else:
        return []


def train_with_ddp(
    rank: int,
    world_size: int,
    curriculum: Curriculum,
    task_map: dict,
    model_name: str,
    agent_type: str,
    user_type: str,
    llm_args_agent: dict,
    llm_args_user: dict,
    learning_rate: float,
    beta: float,
    group_size: int,
    output_dir: str,
    use_grpo: bool,
    verbose: bool,
):
    """
    DDP è®­ç»ƒä¸»å‡½æ•°
    æ¯ä¸ª GPU è¿è¡Œä¸€ä¸ªè¿›ç¨‹

    Args:
        rank: å½“å‰è¿›ç¨‹çš„ rankï¼ˆGPU ç¼–å·ï¼‰
        world_size: æ€»è¿›ç¨‹æ•°ï¼ˆGPU æ•°é‡ï¼‰
        å…¶ä»–å‚æ•°åŒä¸»å‡½æ•°
    """
    # 1. åˆå§‹åŒ– DDP
    setup_ddp(rank, world_size)

    if rank == 0:
        print(f"\n{'='*80}")
        print(f"ğŸš€ å¼€å§‹ DDP è®­ç»ƒ (Rank {rank}/{world_size})")
        print(f"{'='*80}\n")

    # 2. åˆå§‹åŒ– GRPO è®­ç»ƒå™¨ï¼ˆæ¯ä¸ª GPU ä¸€ä¸ªå‰¯æœ¬ï¼‰
    trainer = None
    if use_grpo:
        from tau2.continual.training.grpo_trainer import GRPOContinualTrainer, GRPOTrainingConfig

        grpo_config = GRPOTrainingConfig(
            model_name_or_path=model_name,
            device=f"cuda:{rank}",
            learning_rate=learning_rate,
            beta=beta,
            group_size=group_size,
            output_dir=str(Path(output_dir) / "grpo_checkpoints"),
            torch_dtype="bfloat16",
        )
        trainer = GRPOContinualTrainer(config=grpo_config)
        trainer.load_model()

        # å°†æ¨¡å‹åŒ…è£…ä¸º DDP
        trainer.model = DDP(
            trainer.model,
            device_ids=[rank],
            output_device=rank,
            find_unused_parameters=False
        )

        # å‚è€ƒæ¨¡å‹ä¹Ÿéœ€è¦åœ¨å½“å‰ GPU ä¸Š
        if trainer.ref_model is not None:
            trainer.ref_model = trainer.ref_model.to(f"cuda:{rank}")

        if rank == 0:
            print(f"âœ… GRPO è®­ç»ƒå™¨å·²åˆå§‹åŒ–å¹¶åŒ…è£…ä¸º DDP")

    # 3. åˆ›å»ºç»“æœå­˜å‚¨ï¼ˆä»… rank 0ï¼‰
    results = None
    if rank == 0:
        results = ContinualLearningResults(
            curriculum_id=curriculum.curriculum_id,
            curriculum_name=curriculum.curriculum_name,
            domain=curriculum.domain,
            training_mode=TrainingMode.NONE,
            start_time=datetime.now().isoformat(),
            stage_results=[],
        )

    # 4. éå†æ¯ä¸ªé˜¶æ®µ
    for stage_idx, stage in enumerate(curriculum.stages):
        if rank == 0:
            print(f"\n{'='*80}")
            print(f"ğŸ“ {stage.stage_name} ({stage_idx + 1}/{len(curriculum.stages)})")
            print(f"{'='*80}")

        # ç¡®å®šå½“å‰é˜¶æ®µçš„åŸŸ
        if "airline" in stage.stage_id:
            domain = "airline"
        elif "retail" in stage.stage_id:
            domain = "retail"
        else:
            domain = "telecom"

        # ============================================
        # é˜¶æ®µ 1: å­¦ä¹ é˜¶æ®µï¼ˆDDP å¹¶è¡Œæ”¶é›†ç»éªŒï¼‰
        # ============================================
        learning_runs = []
        if stage.learning_tasks:
            if rank == 0:
                print(f"\nğŸ“– å­¦ä¹ é˜¶æ®µ: {len(stage.learning_tasks)} ä¸ªä»»åŠ¡")

            # è·å–å­¦ä¹ ä»»åŠ¡
            learning_tasks = [task_map[tid] for tid in stage.learning_tasks if tid in task_map]

            # ä½¿ç”¨ DistributedSampler åˆ†é…ä»»åŠ¡åˆ°ä¸åŒ GPU
            # æ¯ä¸ª GPU å¤„ç†ä¸€éƒ¨åˆ†ä»»åŠ¡
            tasks_per_gpu = len(learning_tasks) // world_size
            start_idx = rank * tasks_per_gpu
            end_idx = start_idx + tasks_per_gpu if rank < world_size - 1 else len(learning_tasks)
            my_tasks = learning_tasks[start_idx:end_idx]

            if rank == 0:
                print(f"  æ¯ä¸ª GPU å¤„ç† ~{tasks_per_gpu} ä¸ªä»»åŠ¡")

            # æ¯ä¸ª GPU è¿è¡Œè‡ªå·±çš„ä»»åŠ¡
            my_runs = []
            for task in my_tasks:
                run = run_task_single_gpu(
                    task, domain, agent_type, user_type, model_name,
                    llm_args_agent, llm_args_user, 30, 5, 42, rank
                )
                if run:
                    my_runs.append(run)

            # åŒæ­¥ï¼šç­‰å¾…æ‰€æœ‰ GPU å®Œæˆä»»åŠ¡æ”¶é›†
            dist.barrier()

            # æ”¶é›†æ‰€æœ‰ GPU çš„è¿è¡Œç»“æœ
            learning_runs = gather_all_runs(my_runs, rank, world_size)

            if rank == 0:
                print(f"  âœ… æ”¶é›†åˆ° {len(learning_runs)} ä¸ªç»éªŒ")

            # ============================================
            # é˜¶æ®µ 2: GRPO è®­ç»ƒï¼ˆDDP åŒæ­¥è®­ç»ƒï¼‰
            # ============================================
            if use_grpo and trainer and learning_runs:
                if rank == 0:
                    print(f"\n  ğŸ”§ GRPO DDP è®­ç»ƒ...")

                # è¿‡æ»¤æˆåŠŸçš„ç»éªŒ
                successful_runs = [r for r in learning_runs if r.reward_info and r.reward_info.reward > 0]

                if len(successful_runs) > 0:
                    # åˆ›å»ºæ•°æ®é›†å’Œåˆ†å¸ƒå¼é‡‡æ ·å™¨
                    dataset = ExperienceDataset(successful_runs)
                    sampler = DistributedSampler(
                        dataset,
                        num_replicas=world_size,
                        rank=rank,
                        shuffle=True
                    )

                    # åˆ›å»º DataLoader
                    dataloader = DataLoader(
                        dataset,
                        batch_size=group_size,
                        sampler=sampler,
                        collate_fn=collate_experiences,
                        num_workers=0,  # é¿å…å¤šè¿›ç¨‹å†²çª
                    )

                    # DDP è®­ç»ƒå¾ªç¯
                    total_updates = 0
                    total_loss = 0.0

                    for batch_idx, batch_runs in enumerate(dataloader):
                        # æ¯ä¸ª GPU å¤„ç†è‡ªå·±çš„ batch
                        batch_loss = 0.0

                        for run in batch_runs:
                            # åœ¨å½“å‰ GPU ä¸Šè®­ç»ƒ
                            stats = trainer.train_on_experience(run, stage.stage_id)

                            if stats.get("status") == "updated":
                                batch_loss += stats.get("loss", 0.0)
                                total_updates += 1

                        # è®¡ç®—å¹³å‡ loss
                        if len(batch_runs) > 0:
                            batch_loss /= len(batch_runs)
                            total_loss += batch_loss

                        # DDP ä¼šè‡ªåŠ¨è¿›è¡Œæ¢¯åº¦çš„ All-Reduce
                        # æ‰€æœ‰ GPU çš„æ¢¯åº¦ä¼šè¢«å¹³å‡å¹¶åŒæ­¥

                        if rank == 0 and verbose and batch_idx % 5 == 0:
                            print(f"    Batch {batch_idx}: loss={batch_loss:.4f}, updates={total_updates}")

                    # åŒæ­¥ï¼šç­‰å¾…æ‰€æœ‰ GPU å®Œæˆè®­ç»ƒ
                    dist.barrier()

                    # è®¡ç®—å…¨å±€å¹³å‡ lossï¼ˆä½¿ç”¨ All-Reduceï¼‰
                    avg_loss_tensor = torch.tensor([total_loss / max(len(dataloader), 1)], device=f"cuda:{rank}")
                    dist.all_reduce(avg_loss_tensor, op=dist.ReduceOp.AVG)
                    global_avg_loss = avg_loss_tensor.item()

                    if rank == 0:
                        print(f"  âœ… DDP è®­ç»ƒå®Œæˆ: {total_updates} æ¬¡æ›´æ–°, å…¨å±€å¹³å‡ loss={global_avg_loss:.4f}")

                    # ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆä»… rank 0ï¼‰
                    if rank == 0:
                        checkpoint_path = Path(output_dir) / "grpo_checkpoints" / f"stage_{stage.stage_id}"
                        # ä¿å­˜ DDP æ¨¡å‹æ—¶éœ€è¦è®¿é—® module
                        original_model = trainer.model
                        trainer.model = trainer.model.module  # è§£åŒ… DDP
                        trainer.save_checkpoint(str(checkpoint_path))
                        trainer.model = original_model  # æ¢å¤ DDP
                        print(f"  ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

                    # åŒæ­¥æ£€æŸ¥ç‚¹ä¿å­˜
                    dist.barrier()

                    # æ›´æ–°å‚è€ƒæ¨¡å‹
                    if trainer.ref_model is not None:
                        if rank == 0:
                            # åªåœ¨ rank 0 æ›´æ–°å‚è€ƒæ¨¡å‹
                            trainer.update_reference_model()
                        # å¹¿æ’­å‚è€ƒæ¨¡å‹åˆ°æ‰€æœ‰ GPU
                        dist.barrier()

        # ============================================
        # é˜¶æ®µ 3: è¯„ä¼°é˜¶æ®µï¼ˆDDP å¹¶è¡Œè¯„ä¼°ï¼‰
        # ============================================
        if rank == 0:
            print(f"\nğŸ“Š è¯„ä¼°é˜¶æ®µ: {len(stage.eval_tasks)} ä¸ªä»»åŠ¡")

        eval_tasks = [task_map[tid] for tid in stage.eval_tasks if tid in task_map]

        # åˆ†é…è¯„ä¼°ä»»åŠ¡
        tasks_per_gpu = len(eval_tasks) // world_size
        start_idx = rank * tasks_per_gpu
        end_idx = start_idx + tasks_per_gpu if rank < world_size - 1 else len(eval_tasks)
        my_eval_tasks = eval_tasks[start_idx:end_idx]

        # æ¯ä¸ªä»»åŠ¡è¿è¡Œå¤šæ¬¡ trial
        my_eval_runs = []
        for task in my_eval_tasks:
            for trial in range(stage.num_eval_trials):
                run = run_task_single_gpu(
                    task, domain, agent_type, user_type, model_name,
                    llm_args_agent, llm_args_user, 30, 5, 42 + trial, rank
                )
                if run:
                    my_eval_runs.append(run)

        # åŒæ­¥å¹¶æ”¶é›†è¯„ä¼°ç»“æœ
        dist.barrier()
        eval_runs = gather_all_runs(my_eval_runs, rank, world_size)

        eval_reward = 0.0
        if rank == 0 and eval_runs:
            eval_reward = sum(r.reward_info.reward for r in eval_runs if r.reward_info) / len(eval_runs)
            print(f"  è¯„ä¼°å¥–åŠ±: {eval_reward:.4f}")

        # ============================================
        # é˜¶æ®µ 4: ä¿ç•™ä»»åŠ¡è¯„ä¼°ï¼ˆDDP å¹¶è¡Œï¼‰
        # ============================================
        retention_runs = []
        retention_reward = 0.0

        if stage.retention_tasks:
            if rank == 0:
                print(f"\nğŸ”„ ä¿ç•™ä»»åŠ¡è¯„ä¼°: {len(stage.retention_tasks)} ä¸ªä»»åŠ¡")

            retention_tasks = [task_map[tid] for tid in stage.retention_tasks if tid in task_map]

            # åˆ†é…ä¿ç•™ä»»åŠ¡
            tasks_per_gpu = len(retention_tasks) // world_size
            start_idx = rank * tasks_per_gpu
            end_idx = start_idx + tasks_per_gpu if rank < world_size - 1 else len(retention_tasks)
            my_retention_tasks = retention_tasks[start_idx:end_idx]

            # ç¡®å®šä¿ç•™ä»»åŠ¡çš„åŸŸ
            retention_domain = "airline"
            if retention_tasks:
                tid = retention_tasks[0].id
                if "retail" in tid.lower():
                    retention_domain = "retail"
                elif "telecom" in tid.lower():
                    retention_domain = "telecom"

            my_retention_runs = []
            for task in my_retention_tasks:
                for trial in range(stage.num_eval_trials):
                    run = run_task_single_gpu(
                        task, retention_domain, agent_type, user_type, model_name,
                        llm_args_agent, llm_args_user, 30, 5, 42 + trial, rank
                    )
                    if run:
                        my_retention_runs.append(run)

            dist.barrier()
            retention_runs = gather_all_runs(my_retention_runs, rank, world_size)

            if rank == 0 and retention_runs:
                retention_reward = sum(r.reward_info.reward for r in retention_runs if r.reward_info) / len(retention_runs)
                print(f"  ä¿ç•™ä»»åŠ¡å¥–åŠ±: {retention_reward:.4f}")

        # ============================================
        # ä¿å­˜é˜¶æ®µç»“æœï¼ˆä»… rank 0ï¼‰
        # ============================================
        if rank == 0:
            stage_result = StageResult(
                stage_id=stage.stage_id,
                stage_name=stage.stage_name,
                learning_runs=learning_runs,
                eval_runs=eval_runs,
                retention_runs=retention_runs,
                eval_reward=eval_reward,
                retention_reward=retention_reward,
                pass_k_rates={1: eval_reward, 4: eval_reward},
                new_tool_success_rate=eval_reward,
                tool_performance={},
            )
            results.stage_results.append(stage_result)
            print(f"\nâœ… {stage.stage_name} å®Œæˆ")

        # åŒæ­¥æ‰€æœ‰ GPU
        dist.barrier()

    # ============================================
    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡ï¼ˆä»… rank 0ï¼‰
    # ============================================
    if rank == 0:
        results.end_time = datetime.now().isoformat()

        print("\n" + "=" * 80)
        print("ğŸ“ˆ è®¡ç®—æŒç»­å­¦ä¹ æŒ‡æ ‡...")
        print("=" * 80)

        metrics = compute_continual_metrics(results)
        print(metrics.summary())

        # ä¿å­˜ç»“æœ
        output_path = Path(output_dir)
        results_path = output_path / "results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results.to_dict(), f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {results_path}")

        metrics_path = output_path / "metrics.json"
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(metrics.to_dict(), f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ æŒ‡æ ‡å·²ä¿å­˜: {metrics_path}")

        print("\n" + "=" * 80)
        print("ğŸ‰ DDP è®­ç»ƒå®Œæˆï¼")
        print("=" * 80)

        return results, metrics
    else:
        return None, None


def create_three_domain_curriculum(
    airline_tasks: list[Task],
    retail_tasks: list[Task],
    telecom_tasks: list[Task],
    learning_ratio: float = 0.6,
) -> Curriculum:
    """åˆ›å»ºä¸‰åŸŸæŒç»­å­¦ä¹ è¯¾ç¨‹"""
    def split_tasks(tasks, ratio):
        num_learning = int(len(tasks) * ratio)
        return tasks[:num_learning], tasks[num_learning:]

    airline_learning, airline_eval = split_tasks(airline_tasks, learning_ratio)
    retail_learning, retail_eval = split_tasks(retail_tasks, learning_ratio)
    telecom_learning, telecom_eval = split_tasks(telecom_tasks, learning_ratio)

    stages = [
        LearningStage(
            stage_id="stage_1_airline",
            stage_name="Stage 1: Airline Domain",
            learning_tasks=[t.id for t in airline_learning],
            eval_tasks=[t.id for t in airline_eval],
            retention_tasks=[],
            new_tools=[], available_tools=[],
            num_learning_trials=1, num_eval_trials=4, min_pass_rate=0.5,
        ),
        LearningStage(
            stage_id="stage_2_retail",
            stage_name="Stage 2: Retail Domain",
            learning_tasks=[t.id for t in retail_learning],
            eval_tasks=[t.id for t in retail_eval],
            retention_tasks=[t.id for t in airline_eval[:min(10, len(airline_eval))]],
            new_tools=[], available_tools=[],
            num_learning_trials=1, num_eval_trials=4, min_pass_rate=0.5,
        ),
        LearningStage(
            stage_id="stage_3_telecom",
            stage_name="Stage 3: Telecom Domain",
            learning_tasks=[t.id for t in telecom_learning],
            eval_tasks=[t.id for t in telecom_eval],
            retention_tasks=[t.id for t in airline_eval[:min(10, len(airline_eval))]] +
                          [t.id for t in retail_eval[:min(10, len(retail_eval))]],
            new_tools=[], available_tools=[],
            num_learning_trials=1, num_eval_trials=4, min_pass_rate=0.5,
        ),
    ]

    return Curriculum(
        curriculum_id="three_domain_ddp",
        curriculum_name="Three Domain Continual Learning - DDP",
        domain="multi_domain",
        stages=stages,
        description="DDP training with gradient synchronization",
    )


def main():
    parser = argparse.ArgumentParser(description="ä¸‰åŸŸæŒç»­å­¦ä¹  - DDP åˆ†å¸ƒå¼è®­ç»ƒ")
    parser.add_argument("--airline-tasks", type=str, default="data/tau2/domains/airline/tasks.json")
    parser.add_argument("--retail-tasks", type=str, default="data/tau2/domains/retail/tasks.json")
    parser.add_argument("--telecom-tasks", type=str, default="data/tau2/domains/telecom/tasks_hard_300.json")
    parser.add_argument("--model", type=str, default="Qwen/Qwen3-4B")
    parser.add_argument("--num-gpus", type=int, default=2, help="ä½¿ç”¨çš„ GPU æ•°é‡")
    parser.add_argument("--use-grpo", action="store_true", default=True)
    parser.add_argument("--no-grpo", action="store_true")
    parser.add_argument("--learning-rate", type=float, default=1e-6)
    parser.add_argument("--beta", type=float, default=0.1)
    parser.add_argument("--group-size", type=int, default=4)
    parser.add_argument("--tasks-per-domain", type=int, default=100)
    parser.add_argument("--output-dir", type=str, default="./three_domain_results_ddp")
    parser.add_argument("--quiet", action="store_true")

    args = parser.parse_args()
    use_grpo = args.use_grpo and not args.no_grpo

    # ä¸»è¿›ç¨‹ï¼šåŠ è½½æ•°æ®å’Œåˆ›å»ºè¯¾ç¨‹
    print("=" * 80)
    print("ä¸‰åŸŸæŒç»­å­¦ä¹ è®­ç»ƒ - DDP åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ")
    print("=" * 80)
    print(f"æ¨¡å‹: {args.model}")
    print(f"GPU æ•°é‡: {args.num_gpus}")
    print(f"æ¯åŸŸä»»åŠ¡æ•°: {args.tasks_per_domain}")
    print(f"ä½¿ç”¨ GRPO: {use_grpo}")
    print("=" * 80)
    print()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(args.output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # åŠ è½½ä»»åŠ¡
    print("ğŸ“¥ åŠ è½½ä»»åŠ¡æ•°æ®...")
    airline_tasks = load_tasks_from_json(args.airline_tasks, args.tasks_per_domain)
    retail_tasks = load_tasks_from_json(args.retail_tasks, args.tasks_per_domain)
    telecom_tasks = load_tasks_from_json(args.telecom_tasks, args.tasks_per_domain)

    print(f"âœ… Airline: {len(airline_tasks)} ä¸ªä»»åŠ¡")
    print(f"âœ… Retail: {len(retail_tasks)} ä¸ªä»»åŠ¡")
    print(f"âœ… Telecom: {len(telecom_tasks)} ä¸ªä»»åŠ¡")
    print()

    # åˆ›å»ºè¯¾ç¨‹
    print("ğŸ“š åˆ›å»ºè¯¾ç¨‹...")
    curriculum = create_three_domain_curriculum(
        airline_tasks, retail_tasks, telecom_tasks, learning_ratio=0.6
    )

    curriculum_path = output_path / "curriculum.json"
    curriculum.to_json(curriculum_path)
    print(f"âœ… è¯¾ç¨‹å·²ä¿å­˜: {curriculum_path}")
    print()

    # åˆ›å»ºä»»åŠ¡æ˜ å°„
    all_tasks = airline_tasks + retail_tasks + telecom_tasks
    task_map = {task.id: task for task in all_tasks}

    # é…ç½®å‚æ•°
    agent_type = 'hf_agent'
    user_type = 'hf_user_simulator'
    llm_args_agent = {
        'model_name_or_path': args.model,
        'load_in_4bit': True,
        'torch_dtype': 'bfloat16',
    }
    llm_args_user = {
        'model_name_or_path': args.model,
        'load_in_4bit': True,
        'torch_dtype': 'bfloat16',
    }

    # å¯åŠ¨ DDP è®­ç»ƒ
    # ä½¿ç”¨ mp.spawn å¯åŠ¨å¤šä¸ªè¿›ç¨‹
    try:
        mp.spawn(
            train_with_ddp,
            args=(
                args.num_gpus,
                curriculum,
                task_map,
                args.model,
                agent_type,
                user_type,
                llm_args_agent,
                llm_args_user,
                args.learning_rate,
                args.beta,
                args.group_size,
                args.output_dir,
                use_grpo,
                not args.quiet,
            ),
            nprocs=args.num_gpus,
            join=True
        )

        print("\nâœ… DDP è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        return 0

    except Exception as e:
        print(f"\nâŒ DDP è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
