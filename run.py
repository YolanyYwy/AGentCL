#!/usr/bin/env python3
"""
ä¸‰åŸŸæŒç»­å­¦ä¹ è®­ç»ƒè„šæœ¬
æŒ‰ç…§ Airline â†’ Retail â†’ Telecom é¡ºåºè¿›è¡ŒæŒç»­å­¦ä¹ è®­ç»ƒ
å¹¶è®¡ç®—å‰å‘è¿ç§»ï¼ˆForward Transferï¼‰å’Œåå‘è¿ç§»ï¼ˆBackward Transferï¼‰æŒ‡æ ‡
"""

import argparse
import json
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "src"))

from tau2.continual.training.grpo_trainer import GRPOContinualTrainer, GRPOTrainingConfig
from tau2.continual.curriculum.curriculum import Curriculum
from tau2.continual.curriculum.stage import LearningStage
from tau2.continual.evaluation.evaluator import ContinualLearningEvaluator
from tau2.continual.evaluation.metrics import compute_continual_metrics
from tau2.data_model.continual_results import TrainingMode, ContinualLearningResults
from tau2.run import load_tasks
from loguru import logger


def create_three_domain_curriculum(
    airline_tasks_path: str,
    retail_tasks_path: str,
    telecom_tasks_path: str,
    tasks_per_domain: int = 100,
    learning_ratio: float = 0.6,
) -> Curriculum:
    """
    Args:
        airline_tasks_path: Airline ä»»åŠ¡æ–‡ä»¶è·¯å¾„
        retail_tasks_path: Retail ä»»åŠ¡æ–‡ä»¶è·¯å¾„
        telecom_tasks_path: Telecom ä»»åŠ¡æ–‡ä»¶è·¯å¾„
        tasks_per_domain: æ¯ä¸ªåŸŸä½¿ç”¨çš„ä»»åŠ¡æ•°é‡
        learning_ratio: ç”¨äºå­¦ä¹ çš„ä»»åŠ¡æ¯”ä¾‹ï¼ˆå…¶ä½™ç”¨äºè¯„ä¼°ï¼‰
    Returns:
        Curriculum å¯¹è±¡
    """
    # åŠ è½½ä»»åŠ¡
    with open(airline_tasks_path, 'r', encoding='utf-8') as f:
        airline_tasks = json.load(f)[:tasks_per_domain]

    with open(retail_tasks_path, 'r', encoding='utf-8') as f:
        retail_tasks = json.load(f)[:tasks_per_domain]

    with open(telecom_tasks_path, 'r', encoding='utf-8') as f:
        telecom_tasks = json.load(f)[:tasks_per_domain]

    logger.info(f"åŠ è½½ä»»åŠ¡æ•°é‡: Airline={len(airline_tasks)}, Retail={len(retail_tasks)}, Telecom={len(telecom_tasks)}")

    # è®¡ç®—å­¦ä¹ å’Œè¯„ä¼°ä»»åŠ¡æ•°é‡
    num_learning = int(tasks_per_domain * learning_ratio)
    num_eval = tasks_per_domain - num_learning

    stages = []

    # Stage 1: Airline
    airline_learning_ids = [t['id'] for t in airline_tasks[:num_learning]]
    airline_eval_ids = [t['id'] for t in airline_tasks[num_learning:tasks_per_domain]]

    stage1 = LearningStage(
        stage_id="stage_1_airline",
        stage_name="Stage 1: Airline Domain",
        learning_tasks=airline_learning_ids,
        eval_tasks=airline_eval_ids,
        retention_tasks=[],  # ç¬¬ä¸€é˜¶æ®µæ²¡æœ‰ä¿ç•™ä»»åŠ¡
        new_tools=[],  # å·¥å…·ä¿¡æ¯å°†ä»ä»»åŠ¡ä¸­æå–
        available_tools=[],
        num_learning_trials=1,
        num_eval_trials=4,
        min_pass_rate=0.5,
    )
    stages.append(stage1)

    # Stage 2: Retail (+ Airline retention)
    retail_learning_ids = [t['id'] for t in retail_tasks[:num_learning]]
    retail_eval_ids = [t['id'] for t in retail_tasks[num_learning:tasks_per_domain]]
    airline_retention_ids = airline_eval_ids[:min(10, len(airline_eval_ids))]  # ä¿ç•™10ä¸ªairlineä»»åŠ¡

    stage2 = LearningStage(
        stage_id="stage_2_retail",
        stage_name="Stage 2: Retail Domain",
        learning_tasks=retail_learning_ids,
        eval_tasks=retail_eval_ids,
        retention_tasks=airline_retention_ids,
        new_tools=[],
        available_tools=[],
        num_learning_trials=1,
        num_eval_trials=4,
        min_pass_rate=0.5,
    )
    stages.append(stage2)

    # Stage 3: Telecom (+ Airline + Retail retention)
    telecom_learning_ids = [t['id'] for t in telecom_tasks[:num_learning]]
    telecom_eval_ids = [t['id'] for t in telecom_tasks[num_learning:tasks_per_domain]]
    retail_retention_ids = retail_eval_ids[:min(10, len(retail_eval_ids))]

    stage3 = LearningStage(
        stage_id="stage_3_telecom",
        stage_name="Stage 3: Telecom Domain",
        learning_tasks=telecom_learning_ids,
        eval_tasks=telecom_eval_ids,
        retention_tasks=airline_retention_ids + retail_retention_ids,  # ä¿ç•™å‰ä¸¤ä¸ªåŸŸçš„ä»»åŠ¡
        new_tools=[],
        available_tools=[],
        num_learning_trials=1,
        num_eval_trials=4,
        min_pass_rate=0.5,
    )
    stages.append(stage3)

    curriculum = Curriculum(
        curriculum_id="three_domain_continual",
        curriculum_name="Three Domain Continual Learning (Airline â†’ Retail â†’ Telecom)",
        domain="multi_domain",
        stages=stages,
        description="Sequential training across Airline, Retail, and Telecom domains with GRPO",
    )

    return curriculum


def run_three_domain_training(
    airline_tasks_path: str,
    retail_tasks_path: str,
    telecom_tasks_path: str,
    model_name: str = "Qwen/Qwen3-4B",
    device: str = "cuda",
    learning_rate: float = 1e-6,
    beta: float = 0.1,
    group_size: int = 4,
    tasks_per_domain: int = 100,
    output_dir: str = "./three_domain_results",
    use_grpo: bool = True,
    verbose: bool = True,
):
    """
    è¿è¡Œä¸‰åŸŸæŒç»­å­¦ä¹ è®­ç»ƒ

    Args:
        airline_tasks_path: Airline ä»»åŠ¡æ–‡ä»¶è·¯å¾„
        retail_tasks_path: Retail ä»»åŠ¡æ–‡ä»¶è·¯å¾„
        telecom_tasks_path: Telecom ä»»åŠ¡æ–‡ä»¶è·¯å¾„
        model_name: æ¨¡å‹åç§°
        device: è®¾å¤‡
        learning_rate: å­¦ä¹ ç‡
        beta: KL æƒ©ç½šç³»æ•°
        group_size: GRPO group size
        tasks_per_domain: æ¯ä¸ªåŸŸçš„ä»»åŠ¡æ•°é‡
        output_dir: è¾“å‡ºç›®å½•
        use_grpo: æ˜¯å¦ä½¿ç”¨ GRPO è®­ç»ƒ
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    """
    print("=" * 80)
    print("ä¸‰åŸŸæŒç»­å­¦ä¹ è®­ç»ƒ")
    print("=" * 80)
    print(f"è®­ç»ƒé¡ºåº: Airline â†’ Retail â†’ Telecom")
    print(f"æ¨¡å‹: {model_name}")
    print(f"è®¾å¤‡: {device}")
    print(f"æ¯åŸŸä»»åŠ¡æ•°: {tasks_per_domain}")
    print(f"ä½¿ç”¨ GRPO: {use_grpo}")
    if use_grpo:
        print(f"å­¦ä¹ ç‡: {learning_rate}")
        print(f"Beta: {beta}")
        print(f"Group Size: {group_size}")
    print("=" * 80)
    print()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # åˆ›å»ºè¯¾ç¨‹
    print("ğŸ“š åˆ›å»ºè¯¾ç¨‹...")
    curriculum = create_three_domain_curriculum(
        airline_tasks_path=airline_tasks_path,
        retail_tasks_path=retail_tasks_path,
        telecom_tasks_path=telecom_tasks_path,
        tasks_per_domain=tasks_per_domain,
        learning_ratio=0.6,
    )

    # ä¿å­˜è¯¾ç¨‹é…ç½®
    curriculum_path = output_path / "curriculum.json"
    curriculum.to_json(curriculum_path)
    print(f"âœ… è¯¾ç¨‹å·²ä¿å­˜: {curriculum_path}")
    print()

    # åŠ è½½æ‰€æœ‰ä»»åŠ¡
    print("ğŸ“¥ åŠ è½½ä»»åŠ¡æ•°æ®...")
    all_tasks = {}

    with open(airline_tasks_path, 'r', encoding='utf-8') as f:
        airline_tasks = json.load(f)[:tasks_per_domain]
        for task in airline_tasks:
            all_tasks[task['id']] = task

    with open(retail_tasks_path, 'r', encoding='utf-8') as f:
        retail_tasks = json.load(f)[:tasks_per_domain]
        for task in retail_tasks:
            all_tasks[task['id']] = task

    with open(telecom_tasks_path, 'r', encoding='utf-8') as f:
        telecom_tasks = json.load(f)[:tasks_per_domain]
        for task in telecom_tasks:
            all_tasks[task['id']] = task

    print(f"âœ… æ€»ä»»åŠ¡æ•°: {len(all_tasks)}")
    print()

    # åˆå§‹åŒ– GRPO è®­ç»ƒå™¨ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
    trainer = None
    if use_grpo:
        print("ğŸ”§ åˆå§‹åŒ– GRPO è®­ç»ƒå™¨...")
        grpo_config = GRPOTrainingConfig(
            model_name_or_path=model_name,
            device=device,
            learning_rate=learning_rate,
            beta=beta,
            group_size=group_size,
            output_dir=str(output_path / "grpo_checkpoints"),
        )
        trainer = GRPOContinualTrainer(config=grpo_config)
        trainer.load_model()
        print("âœ… GRPO è®­ç»ƒå™¨å·²åˆå§‹åŒ–")
        print()

    # åˆ›å»ºè¯„ä¼°å™¨
    print("ğŸ”§ åˆå§‹åŒ–è¯„ä¼°å™¨...")
    evaluator = ContinualLearningEvaluator(
        curriculum=curriculum,
        domain="multi_domain",
        agent_llm=None,  # ä½¿ç”¨æœ¬åœ°æ¨¡å‹
        user_llm=None,
        agent_type='hf_agent' if use_grpo else 'llm_agent',
        user_type='hf_user_simulator' if use_grpo else 'user_simulator',
        llm_args_agent={
            'model_name_or_path': model_name,
            'load_in_4bit': False,
        } if use_grpo else {},
        llm_args_user={
            'model_name_or_path': model_name,
            'load_in_4bit': False,
        } if use_grpo else {},
        max_steps=30,
        max_errors=5,
        seed=42,
        training_mode=TrainingMode.NONE,  # æˆ‘ä»¬æ‰‹åŠ¨æ§åˆ¶è®­ç»ƒ
        verbose=verbose,
    )

    # è®¾ç½®ä»»åŠ¡
    evaluator.set_tasks(list(all_tasks.values()))
    print("âœ… è¯„ä¼°å™¨å·²åˆå§‹åŒ–")
    print()

    # è¿è¡ŒæŒç»­å­¦ä¹ è®­ç»ƒ
    print("=" * 80)
    print("ğŸš€ å¼€å§‹æŒç»­å­¦ä¹ è®­ç»ƒ...")
    print("=" * 80)
    print()

    results = ContinualLearningResults(
        curriculum_id=curriculum.curriculum_id,
        curriculum_name=curriculum.curriculum_name,
        domain=curriculum.domain,
        training_mode=TrainingMode.NONE,
        start_time=datetime.now().isoformat(),
        stage_results=[],
    )

    for stage_idx, stage in enumerate(curriculum.stages):
        print(f"\n{'='*80}")
        print(f"ğŸ“ {stage.stage_name} ({stage_idx + 1}/{len(curriculum.stages)})")
        print(f"{'='*80}")
        print(f"å­¦ä¹ ä»»åŠ¡: {len(stage.learning_tasks)}")
        print(f"è¯„ä¼°ä»»åŠ¡: {len(stage.eval_tasks)}")
        print(f"ä¿ç•™ä»»åŠ¡: {len(stage.retention_tasks)}")
        print()

        # 1. å­¦ä¹ é˜¶æ®µ
        if use_grpo and trainer and stage.learning_tasks:
            print(f"ğŸ“– å­¦ä¹ é˜¶æ®µ: åœ¨ {len(stage.learning_tasks)} ä¸ªä»»åŠ¡ä¸Šè®­ç»ƒ...")

            # è¿è¡Œå­¦ä¹ ä»»åŠ¡å¹¶æ”¶é›†è½¨è¿¹
            learning_runs = evaluator._run_tasks(
                task_ids=stage.learning_tasks,
                num_trials=stage.num_learning_trials,
                stage_id=stage.stage_id,
                phase="learning",
            )

            # ä½¿ç”¨ GRPO è®­ç»ƒ
            for run in learning_runs:
                if run.reward_info and run.reward_info.reward > 0:
                    stats = trainer.train_on_experience(run, stage.stage_id)
                    if verbose and stats.get("status") == "updated":
                        print(f"  âœ“ æ›´æ–°: loss={stats.get('loss', 0):.4f}, total={stats.get('total_updates', 0)}")

            # ä¿å­˜æ£€æŸ¥ç‚¹
            checkpoint_path = output_path / "grpo_checkpoints" / f"stage_{stage.stage_id}"
            trainer.save_checkpoint(str(checkpoint_path))
            print(f"  ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

            # æ›´æ–°å‚è€ƒæ¨¡å‹
            trainer.update_reference_model()
            print(f"  ğŸ”„ å‚è€ƒæ¨¡å‹å·²æ›´æ–°")

        # 2. è¯„ä¼°é˜¶æ®µ
        print(f"\nğŸ“Š è¯„ä¼°é˜¶æ®µ: åœ¨ {len(stage.eval_tasks)} ä¸ªä»»åŠ¡ä¸Šè¯„ä¼°...")
        eval_runs = evaluator._run_tasks(
            task_ids=stage.eval_tasks,
            num_trials=stage.num_eval_trials,
            stage_id=stage.stage_id,
            phase="eval",
        )

        eval_reward = sum(r.reward_info.reward for r in eval_runs if r.reward_info) / len(eval_runs) if eval_runs else 0.0
        print(f"  è¯„ä¼°å¥–åŠ±: {eval_reward:.4f}")

        # 3. ä¿ç•™ä»»åŠ¡è¯„ä¼°ï¼ˆæµ‹è¯•åå‘è¿ç§»ï¼‰
        retention_runs = []
        retention_reward = 0.0
        if stage.retention_tasks:
            print(f"\nğŸ”„ ä¿ç•™ä»»åŠ¡è¯„ä¼°: åœ¨ {len(stage.retention_tasks)} ä¸ªä»»åŠ¡ä¸Šè¯„ä¼°...")
            retention_runs = evaluator._run_tasks(
                task_ids=stage.retention_tasks,
                num_trials=stage.num_eval_trials,
                stage_id=stage.stage_id,
                phase="retention",
            )
            retention_reward = sum(r.reward_info.reward for r in retention_runs if r.reward_info) / len(retention_runs) if retention_runs else 0.0
            print(f"  ä¿ç•™ä»»åŠ¡å¥–åŠ±: {retention_reward:.4f}")

        # ä¿å­˜é˜¶æ®µç»“æœ
        from tau2.data_model.continual_results import StageResult
        stage_result = StageResult(
            stage_id=stage.stage_id,
            stage_name=stage.stage_name,
            learning_runs=learning_runs if use_grpo else [],
            eval_runs=eval_runs,
            retention_runs=retention_runs,
            eval_reward=eval_reward,
            retention_reward=retention_reward,
            pass_k_rates={1: eval_reward, 4: eval_reward},  # ç®€åŒ–ç‰ˆ
            new_tool_success_rate=eval_reward,
            tool_performance={},
        )
        results.stage_results.append(stage_result)

        print(f"\nâœ… {stage.stage_name} å®Œæˆ")

    # å®Œæˆè®­ç»ƒ
    results.end_time = datetime.now().isoformat()

    # è®¡ç®—æŒç»­å­¦ä¹ æŒ‡æ ‡
    print("\n" + "=" * 80)
    print("ğŸ“ˆ è®¡ç®—æŒç»­å­¦ä¹ æŒ‡æ ‡...")
    print("=" * 80)

    metrics = compute_continual_metrics(results)

    # æ‰“å°æŒ‡æ ‡
    print(metrics.summary())

    # ä¿å­˜ç»“æœ
    results_path = output_path / "results.json"
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results.to_dict(), f, indent=2, ensure_ascii=False)
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {results_path}")

    metrics_path = output_path / "metrics.json"
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(metrics.to_dict(), f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ æŒ‡æ ‡å·²ä¿å­˜: {metrics_path}")

    print("\n" + "=" * 80)
    print("ğŸ‰ ä¸‰åŸŸæŒç»­å­¦ä¹ è®­ç»ƒå®Œæˆï¼")
    print("=" * 80)

    return results, metrics


def main():
    parser = argparse.ArgumentParser(
        description="ä¸‰åŸŸæŒç»­å­¦ä¹ è®­ç»ƒ (Airline â†’ Retail â†’ Telecom)"
    )

    # ä»»åŠ¡æ–‡ä»¶è·¯å¾„
    parser.add_argument(
        "--airline-tasks",
        type=str,
        default="data/tau2/domains/airline/tasks.json",
        help="Airline ä»»åŠ¡æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--retail-tasks",
        type=str,
        default="data/tau2/domains/retail/tasks.json",
        help="Retail ä»»åŠ¡æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--telecom-tasks",
        type=str,
        default="data/tau2/domains/telecom/tasks_hard_300.json",
        help="Telecom ä»»åŠ¡æ–‡ä»¶è·¯å¾„",
    )

    # æ¨¡å‹é…ç½®
    parser.add_argument(
        "--model",
        type=str,
        default="Qwen/Qwen3-4B",
        help="æ¨¡å‹åç§°",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        choices=["cuda", "cpu", "auto"],
        help="è®¾å¤‡",
    )

    # GRPO é…ç½®
    parser.add_argument(
        "--use-grpo",
        action="store_true",
        default=True,
        help="ä½¿ç”¨ GRPO è®­ç»ƒ",
    )
    parser.add_argument(
        "--no-grpo",
        action="store_true",
        help="ä¸ä½¿ç”¨ GRPO è®­ç»ƒï¼ˆä»…è¯„ä¼°ï¼‰",
    )
    parser.add_argument(
        "--learning-rate",
        type=float,
        default=1e-6,
        help="å­¦ä¹ ç‡",
    )
    parser.add_argument(
        "--beta",
        type=float,
        default=0.1,
        help="KL æƒ©ç½šç³»æ•°",
    )
    parser.add_argument(
        "--group-size",
        type=int,
        default=4,
        help="GRPO group size",
    )

    # ä»»åŠ¡é…ç½®
    parser.add_argument(
        "--tasks-per-domain",
        type=int,
        default=100,
        help="æ¯ä¸ªåŸŸä½¿ç”¨çš„ä»»åŠ¡æ•°é‡",
    )

    # è¾“å‡ºé…ç½®
    parser.add_argument(
        "--output-dir",
        type=str,
        default="./three_domain_results",
        help="è¾“å‡ºç›®å½•",
    )
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="é™é»˜æ¨¡å¼",
    )

    args = parser.parse_args()

    # å¤„ç† no-grpo æ ‡å¿—
    use_grpo = args.use_grpo and not args.no_grpo

    try:
        results, metrics = run_three_domain_training(
            airline_tasks_path=args.airline_tasks,
            retail_tasks_path=args.retail_tasks,
            telecom_tasks_path=args.telecom_tasks,
            model_name=args.model,
            device=args.device,
            learning_rate=args.learning_rate,
            beta=args.beta,
            group_size=args.group_size,
            tasks_per_domain=args.tasks_per_domain,
            output_dir=args.output_dir,
            use_grpo=use_grpo,
            verbose=not args.quiet,
        )

        print("\nâœ… è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print(f"\nå…³é”®æŒ‡æ ‡:")
        print(f"  å¹³å‡å¥–åŠ±: {metrics.average_reward:.4f}")
        print(f"  å‰å‘è¿ç§»: {metrics.forward_transfer:.4f}")
        print(f"  åå‘è¿ç§»: {metrics.backward_transfer:.4f}")
        print(f"  å¹³å‡é—å¿˜: {metrics.average_forgetting:.4f}")

        return 0

    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
