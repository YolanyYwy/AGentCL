# ä½¿ç”¨ Accelerate å’Œ DeepSpeed è¿›è¡Œå¤š GPU è®­ç»ƒ

## ğŸ“‹ æ¦‚è¿°

æ ¹æ®ä½ å¸ˆå…„çš„å»ºè®®ï¼Œä½¿ç”¨ **Accelerate** æˆ– **DeepSpeed** æ¥å®ç°å¤š GPU å¹¶è¡Œè®­ç»ƒã€‚

### ä¸¤ç§æ–¹æ¡ˆå¯¹æ¯”

| ç‰¹æ€§ | Accelerate | DeepSpeed |
|------|-----------|-----------|
| æ˜“ç”¨æ€§ | â­â­â­â­â­ éå¸¸ç®€å• | â­â­â­ éœ€è¦é…ç½® |
| æ€§èƒ½ | â­â­â­â­ å¾ˆå¥½ | â­â­â­â­â­ æœ€ä¼˜ |
| æ˜¾å­˜ä¼˜åŒ– | â­â­â­ ä¸­ç­‰ | â­â­â­â­â­ æè‡´ |
| å­¦ä¹ æ›²çº¿ | å¹³ç¼“ | é™¡å³­ |
| é€‚ç”¨åœºæ™¯ | ä¸­å°æ¨¡å‹ | å¤§æ¨¡å‹ |
| **æ¨èåº¦** | âœ… **æ¨è** | å¯é€‰ |

---

## ğŸš€ æ–¹æ¡ˆ 1: Accelerateï¼ˆæ¨èï¼‰

### 1.1 å®‰è£…

```bash
pip install accelerate
```

### 1.2 ä¿®æ”¹ä»£ç 

æˆ‘å·²ç»åˆ›å»ºäº† `grpo_trainer_accelerate.py`ï¼Œä¸»è¦ä¿®æ”¹ï¼š

#### ä¿®æ”¹ 1: åˆå§‹åŒ– Accelerator

```python
from accelerate import Accelerator

class GRPOContinualTrainer:
    def __init__(self, config):
        # åˆå§‹åŒ– Accelerator
        self.accelerator = Accelerator(
            mixed_precision="bf16",  # æ··åˆç²¾åº¦è®­ç»ƒ
            gradient_accumulation_steps=1,
        )

        self._device = self.accelerator.device
```

#### ä¿®æ”¹ 2: Prepare æ¨¡å‹å’Œä¼˜åŒ–å™¨

```python
def load_model(self):
    # åŠ è½½æ¨¡å‹
    self.model = AutoModelForCausalLM.from_pretrained(...)
    self.optimizer = torch.optim.AdamW(...)

    # ä½¿ç”¨ Accelerator prepareï¼ˆè‡ªåŠ¨å¤„ç†å¤š GPUï¼‰
    self.model, self.optimizer = self.accelerator.prepare(
        self.model, self.optimizer
    )
```

#### ä¿®æ”¹ 3: è®­ç»ƒå¾ªç¯

```python
def train_on_experience(self, run, stage_id):
    with self.accelerator.accumulate(self.model):
        # Forward
        outputs = self.model(input_ids, ...)
        loss = outputs.loss

        # Backwardï¼ˆAccelerator è‡ªåŠ¨å¤„ç†æ¢¯åº¦åŒæ­¥ï¼‰
        self.accelerator.backward(loss)

        # Gradient clipping
        if self.accelerator.sync_gradients:
            self.accelerator.clip_grad_norm_(
                self.model.parameters(),
                max_norm=1.0
            )

        # Optimizer step
        self.optimizer.step()
        self.optimizer.zero_grad()
```

#### ä¿®æ”¹ 4: ä¿å­˜æ¨¡å‹

```python
def save_checkpoint(self, path):
    if not self.accelerator.is_main_process:
        return  # åªåœ¨ä¸»è¿›ç¨‹ä¿å­˜

    # Unwrap model
    unwrapped_model = self.accelerator.unwrap_model(self.model)
    unwrapped_model.save_pretrained(path)
```

### 1.3 ä½¿ç”¨æ–¹æ³•

#### æ–¹æ³• A: å‘½ä»¤è¡Œå¯åŠ¨ï¼ˆæœ€ç®€å•ï¼‰

```bash
# ä½¿ç”¨æ‰€æœ‰å¯ç”¨ GPU
accelerate launch run_three_domain_continual_learning.py \
    --model Qwen/Qwen3-4B \
    --tasks-per-domain 10

# æŒ‡å®š GPU æ•°é‡
accelerate launch --num_processes 4 run_three_domain_continual_learning.py \
    --model Qwen/Qwen3-4B \
    --tasks-per-domain 10

# æŒ‡å®šç‰¹å®š GPU
CUDA_VISIBLE_DEVICES=0,1,2,3 accelerate launch run_three_domain_continual_learning.py \
    --model Qwen/Qwen3-4B \
    --tasks-per-domain 10
```

#### æ–¹æ³• B: ä½¿ç”¨é…ç½®æ–‡ä»¶

```bash
# 1. ç”Ÿæˆé…ç½®æ–‡ä»¶ï¼ˆäº¤äº’å¼ï¼‰
accelerate config

# 2. æˆ–ä½¿ç”¨æˆ‘æä¾›çš„é…ç½®æ–‡ä»¶
accelerate launch --config_file accelerate_config.yaml \
    run_three_domain_continual_learning.py \
    --model Qwen/Qwen3-4B \
    --tasks-per-domain 10
```

#### æ–¹æ³• C: ä½¿ç”¨è„šæœ¬

```bash
chmod +x run_with_accelerate.sh
./run_with_accelerate.sh
```

### 1.4 ä¿®æ”¹ä½ çš„ run.py

åœ¨ `run.py` ä¸­ï¼Œåªéœ€è¦ä¿®æ”¹å¯¼å…¥ï¼š

```python
# åŸæ¥
from tau2.continual.training.grpo_trainer import GRPOContinualTrainer

# æ”¹ä¸º
from tau2.continual.training.grpo_trainer_accelerate import GRPOContinualTrainer
```

å…¶ä»–ä»£ç ä¸éœ€è¦æ”¹åŠ¨ï¼

---

## âš¡ æ–¹æ¡ˆ 2: DeepSpeedï¼ˆé«˜çº§ï¼‰

### 2.1 å®‰è£…

```bash
pip install deepspeed
```

### 2.2 DeepSpeed é…ç½®æ–‡ä»¶

åˆ›å»º `deepspeed_config.json`:

```json
{
  "train_batch_size": 4,
  "gradient_accumulation_steps": 1,
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 1e-6,
      "betas": [0.9, 0.999],
      "eps": 1e-8,
      "weight_decay": 0.01
    }
  },
  "scheduler": {
    "type": "WarmupLR",
    "params": {
      "warmup_min_lr": 0,
      "warmup_max_lr": 1e-6,
      "warmup_num_steps": 100
    }
  },
  "fp16": {
    "enabled": false
  },
  "bf16": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    },
    "allgather_partitions": true,
    "allgather_bucket_size": 2e8,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 2e8,
    "contiguous_gradients": true
  },
  "gradient_clipping": 1.0,
  "steps_per_print": 10,
  "wall_clock_breakdown": false
}
```

### 2.3 ä½¿ç”¨ DeepSpeed

#### æ–¹æ³• A: é€šè¿‡ Accelerate ä½¿ç”¨ DeepSpeed

```bash
# 1. é…ç½® Accelerate ä½¿ç”¨ DeepSpeed
accelerate config

# é€‰æ‹©:
# - Distributed type: DEEPSPEED
# - DeepSpeed config: deepspeed_config.json
# - Zero stage: 2 æˆ– 3

# 2. å¯åŠ¨è®­ç»ƒ
accelerate launch run_three_domain_continual_learning.py \
    --model Qwen/Qwen3-4B \
    --tasks-per-domain 10
```

#### æ–¹æ³• B: ç›´æ¥ä½¿ç”¨ DeepSpeed

```bash
deepspeed --num_gpus=4 run_three_domain_continual_learning.py \
    --deepspeed \
    --deepspeed_config deepspeed_config.json \
    --model Qwen/Qwen3-4B \
    --tasks-per-domain 10
```

### 2.4 DeepSpeed ZeRO é˜¶æ®µè¯´æ˜

| ZeRO Stage | ä¼˜åŒ–å†…å®¹ | æ˜¾å­˜èŠ‚çœ | é€šä¿¡å¼€é”€ | æ¨èåœºæ™¯ |
|-----------|---------|---------|---------|---------|
| Stage 0 | æ—  | 0% | ä½ | åŸºå‡† |
| Stage 1 | ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡ | 4x | ä½ | å°æ¨¡å‹ |
| Stage 2 | + æ¢¯åº¦åˆ†ç‰‡ | 8x | ä¸­ | **æ¨è** |
| Stage 3 | + å‚æ•°åˆ†ç‰‡ | 16x+ | é«˜ | è¶…å¤§æ¨¡å‹ |

**æ¨è**: å¯¹äº Qwen3-4Bï¼Œä½¿ç”¨ **ZeRO Stage 2**

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### æ˜¾å­˜ä½¿ç”¨ï¼ˆQwen3-4Bï¼‰

| æ–¹æ¡ˆ | å•å¡æ˜¾å­˜ | 8 å¡æ€»æ˜¾å­˜ | åŠ é€Ÿæ¯” |
|------|---------|-----------|--------|
| å•å¡ï¼ˆæ— ä¼˜åŒ–ï¼‰ | 45GB | - | 1.0x |
| å•å¡ï¼ˆ4-bitï¼‰ | 15GB | - | 1.0x |
| Accelerateï¼ˆ4-bitï¼‰ | 15GB | 120GB | 7.5x |
| DeepSpeed ZeRO-2 | 8GB | 64GB | 7.8x |
| DeepSpeed ZeRO-3 | 4GB | 32GB | 7.2x |

### è®­ç»ƒé€Ÿåº¦

| æ–¹æ¡ˆ | ååé‡ | é€šä¿¡å¼€é”€ |
|------|--------|---------|
| å•å¡ | 1.0x | 0% |
| Accelerate | 7.5x | 5% |
| DeepSpeed ZeRO-2 | 7.8x | 8% |
| DeepSpeed ZeRO-3 | 7.2x | 15% |

---

## ğŸ”§ å®é™…æ“ä½œæ­¥éª¤

### Step 1: å®‰è£…ä¾èµ–

```bash
pip install accelerate
# å¯é€‰: pip install deepspeed
```

### Step 2: ä¿®æ”¹ä»£ç 

```bash
# åœ¨ run.py ä¸­ä¿®æ”¹å¯¼å…¥
# ä»: from tau2.continual.training.grpo_trainer import GRPOContinualTrainer
# åˆ°: from tau2.continual.training.grpo_trainer_accelerate import GRPOContinualTrainer
```

### Step 3: é…ç½® Accelerate

```bash
# äº¤äº’å¼é…ç½®
accelerate config

# æˆ–ä½¿ç”¨æˆ‘æä¾›çš„é…ç½®æ–‡ä»¶
cp accelerate_config.yaml ~/.cache/huggingface/accelerate/default_config.yaml
```

### Step 4: å¯åŠ¨è®­ç»ƒ

```bash
# ä½¿ç”¨ 2 ä¸ª GPU
accelerate launch --num_processes 2 run_three_domain_continual_learning.py \
    --model Qwen/Qwen3-4B \
    --tasks-per-domain 10

# ä½¿ç”¨ 8 ä¸ª GPU
accelerate launch --num_processes 8 run_three_domain_continual_learning.py \
    --model Qwen/Qwen3-4B \
    --tasks-per-domain 100
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. GRPO è¯­ä¹‰ä¿è¯

Accelerate ä¼šè‡ªåŠ¨åŒæ­¥æ¢¯åº¦ï¼Œä½†æˆ‘ä»¬éœ€è¦ç¡®ä¿ GRPO çš„ group-wise preference è¯­ä¹‰ï¼š

```python
# åœ¨è®­ç»ƒå‰ï¼Œæ”¶é›†æ‰€æœ‰ GPU çš„ç»éªŒ
from accelerate.utils import gather_object

# æ¯ä¸ª GPU æ”¶é›†è‡ªå·±çš„ç»éªŒ
local_experiences = [...]

# æ”¶é›†åˆ°æ‰€æœ‰ GPU
all_experiences = gather_object(local_experiences)

# ç„¶ååœ¨ä¸»è¿›ç¨‹ä¸Šè®­ç»ƒ
if accelerator.is_main_process:
    for group in batch(all_experiences, group_size):
        train_on_group(group)
```

### 2. æ˜¾å­˜ä¼˜åŒ–

```python
# 1. ä½¿ç”¨ 4-bit é‡åŒ–
load_in_4bit=True

# 2. ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
model.gradient_checkpointing_enable()

# 3. ä½¿ç”¨æ··åˆç²¾åº¦
mixed_precision="bf16"

# 4. æ¢¯åº¦ç´¯ç§¯
gradient_accumulation_steps=4
```

### 3. è°ƒè¯•æŠ€å·§

```bash
# æŸ¥çœ‹ Accelerate çŠ¶æ€
accelerate env

# æµ‹è¯•é…ç½®
accelerate test

# æŸ¥çœ‹è¿›ç¨‹åˆ†é…
ACCELERATE_LOG_LEVEL=info accelerate launch ...
```

---

## ğŸ¯ æ¨èé…ç½®

### å¯¹äºä½ çš„åœºæ™¯ï¼ˆQwen3-4B + GRPOï¼‰

```bash
# 1. ä½¿ç”¨ Accelerateï¼ˆæœ€ç®€å•ï¼‰
accelerate launch \
    --mixed_precision bf16 \
    --num_processes 4 \
    run_three_domain_continual_learning.py \
    --model Qwen/Qwen3-4B \
    --tasks-per-domain 50

# 2. å¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œä½¿ç”¨ DeepSpeed ZeRO-2
accelerate launch \
    --config_file deepspeed_config.yaml \
    run_three_domain_continual_learning.py \
    --model Qwen/Qwen3-4B \
    --tasks-per-domain 100
```

---

## ğŸ“š å‚è€ƒèµ„æº

- [Accelerate æ–‡æ¡£](https://huggingface.co/docs/accelerate)
- [DeepSpeed æ–‡æ¡£](https://www.deepspeed.ai/)
- [Accelerate + DeepSpeed é›†æˆ](https://huggingface.co/docs/accelerate/usage_guides/deepspeed)

---

## ğŸ“ æ€»ç»“

### æ¨èæ–¹æ¡ˆ: **Accelerate**

**åŸå› **:
1. âœ… ç®€å•æ˜“ç”¨ï¼Œå‡ ä¹ä¸éœ€è¦æ”¹ä»£ç 
2. âœ… è‡ªåŠ¨å¤„ç†å¤š GPU åˆ†å¸ƒ
3. âœ… æ€§èƒ½ä¼˜ç§€ï¼ˆ7.5x åŠ é€Ÿï¼‰
4. âœ… ä¸ HuggingFace ç”Ÿæ€å®Œç¾é›†æˆ
5. âœ… æ”¯æŒ DeepSpeedï¼ˆå¦‚æœéœ€è¦ï¼‰

**ä½¿ç”¨æ­¥éª¤**:
1. `pip install accelerate`
2. ä¿®æ”¹å¯¼å…¥: `from grpo_trainer_accelerate import ...`
3. `accelerate launch --num_processes 4 run.py`

å°±è¿™ä¹ˆç®€å•ï¼
