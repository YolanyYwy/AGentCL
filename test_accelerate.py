#!/usr/bin/env python3
"""
Accelerate å¤šå¡è®­ç»ƒæµ‹è¯•è„šæœ¬
æµ‹è¯• Accelerate æ˜¯å¦æ­£ç¡®é…ç½®å¹¶èƒ½æ­£å¸¸è¿è¡Œ
"""

import torch
import torch.nn as nn
from accelerate import Accelerator
import time


def test_accelerate_basic():
    """æµ‹è¯• 1: Accelerate åŸºç¡€åŠŸèƒ½"""
    print("\n" + "="*80)
    print("æµ‹è¯• 1: Accelerate åŸºç¡€åŠŸèƒ½")
    print("="*80)

    try:
        accelerator = Accelerator()

        print(f"âœ… Accelerator åˆå§‹åŒ–æˆåŠŸ")
        print(f"  - è¿›ç¨‹æ•°é‡: {accelerator.num_processes}")
        print(f"  - å½“å‰è¿›ç¨‹: {accelerator.process_index}")
        print(f"  - è®¾å¤‡: {accelerator.device}")
        print(f"  - æ˜¯å¦ä¸»è¿›ç¨‹: {accelerator.is_main_process}")
        print(f"  - æ··åˆç²¾åº¦: {accelerator.mixed_precision}")

        return True
    except Exception as e:
        print(f"âŒ Accelerator åˆå§‹åŒ–å¤±è´¥: {e}")
        return False


def test_model_distribution():
    """æµ‹è¯• 2: æ¨¡å‹åˆ†å¸ƒ"""
    print("\n" + "="*80)
    print("æµ‹è¯• 2: æ¨¡å‹åˆ†å¸ƒåˆ°å¤š GPU")
    print("="*80)

    try:
        accelerator = Accelerator()

        # åˆ›å»ºç®€å•æ¨¡å‹
        model = nn.Sequential(
            nn.Linear(100, 256),
            nn.ReLU(),
            nn.Linear(256, 10)
        )

        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

        # Prepare with Accelerator
        model, optimizer = accelerator.prepare(model, optimizer)

        if accelerator.is_main_process:
            print(f"âœ… æ¨¡å‹å·²åˆ†å¸ƒåˆ°æ‰€æœ‰ GPU")
            print(f"  - æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device}")

        return True
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ†å¸ƒå¤±è´¥: {e}")
        return False


def test_gradient_sync():
    """æµ‹è¯• 3: æ¢¯åº¦åŒæ­¥"""
    print("\n" + "="*80)
    print("æµ‹è¯• 3: æ¢¯åº¦åŒæ­¥")
    print("="*80)

    try:
        accelerator = Accelerator()

        # åˆ›å»ºæ¨¡å‹
        model = nn.Linear(10, 1)
        optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

        model, optimizer = accelerator.prepare(model, optimizer)

        # åˆ›å»ºä¸åŒçš„æ•°æ®ï¼ˆæ¯ä¸ª GPU ä¸åŒï¼‰
        x = torch.randn(4, 10, device=accelerator.device) * (accelerator.process_index + 1)
        y = torch.randn(4, 1, device=accelerator.device)

        # å‰å‘ä¼ æ’­
        output = model(x)
        loss = nn.functional.mse_loss(output, y)

        # åå‘ä¼ æ’­
        accelerator.backward(loss)

        # è·å–æ¢¯åº¦
        grad_before = model.weight.grad.clone()

        # ä¼˜åŒ–å™¨æ­¥éª¤
        optimizer.step()
        optimizer.zero_grad()

        # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹
        accelerator.wait_for_everyone()

        if accelerator.is_main_process:
            print(f"âœ… æ¢¯åº¦åŒæ­¥æˆåŠŸ")
            print(f"  - Loss: {loss.item():.4f}")
            print(f"  - æ¢¯åº¦èŒƒæ•°: {grad_before.norm().item():.4f}")

        return True
    except Exception as e:
        print(f"âŒ æ¢¯åº¦åŒæ­¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_data_distribution():
    """æµ‹è¯• 4: æ•°æ®åˆ†å¸ƒ"""
    print("\n" + "="*80)
    print("æµ‹è¯• 4: æ•°æ®åˆ†å¸ƒ")
    print("="*80)

    try:
        accelerator = Accelerator()

        # åˆ›å»ºæ•°æ®
        data = list(range(100))

        # æ¯ä¸ªè¿›ç¨‹å¤„ç†ä¸åŒçš„æ•°æ®
        per_device = len(data) // accelerator.num_processes
        start_idx = accelerator.process_index * per_device
        end_idx = start_idx + per_device

        my_data = data[start_idx:end_idx]

        print(f"[è¿›ç¨‹ {accelerator.process_index}] å¤„ç†æ•°æ®: {len(my_data)} ä¸ªæ ·æœ¬")
        print(f"[è¿›ç¨‹ {accelerator.process_index}] æ•°æ®èŒƒå›´: {my_data[0]} - {my_data[-1]}")

        # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹
        accelerator.wait_for_everyone()

        if accelerator.is_main_process:
            print(f"âœ… æ•°æ®åˆ†å¸ƒæˆåŠŸ")

        return True
    except Exception as e:
        print(f"âŒ æ•°æ®åˆ†å¸ƒå¤±è´¥: {e}")
        return False


def test_gather_operation():
    """æµ‹è¯• 5: Gather æ“ä½œ"""
    print("\n" + "="*80)
    print("æµ‹è¯• 5: Gather æ“ä½œï¼ˆæ”¶é›†æ‰€æœ‰ GPU çš„æ•°æ®ï¼‰")
    print("="*80)

    try:
        accelerator = Accelerator()

        # æ¯ä¸ªè¿›ç¨‹åˆ›å»ºä¸åŒçš„æ•°æ®
        local_data = torch.tensor([accelerator.process_index], device=accelerator.device)

        # Gather åˆ°æ‰€æœ‰è¿›ç¨‹
        gathered_data = accelerator.gather(local_data)

        if accelerator.is_main_process:
            print(f"âœ… Gather æ“ä½œæˆåŠŸ")
            print(f"  - æ”¶é›†çš„æ•°æ®: {gathered_data.cpu().tolist()}")

        return True
    except Exception as e:
        print(f"âŒ Gather æ“ä½œå¤±è´¥: {e}")
        return False


def test_save_load():
    """æµ‹è¯• 6: æ¨¡å‹ä¿å­˜å’ŒåŠ è½½"""
    print("\n" + "="*80)
    print("æµ‹è¯• 6: æ¨¡å‹ä¿å­˜å’ŒåŠ è½½")
    print("="*80)

    try:
        accelerator = Accelerator()

        # åˆ›å»ºæ¨¡å‹
        model = nn.Linear(10, 5)
        model, = accelerator.prepare(model)

        # ä¿å­˜æ¨¡å‹ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
        if accelerator.is_main_process:
            unwrapped_model = accelerator.unwrap_model(model)
            torch.save(unwrapped_model.state_dict(), "/tmp/test_model.pt")
            print(f"âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ: /tmp/test_model.pt")

        # ç­‰å¾…ä¿å­˜å®Œæˆ
        accelerator.wait_for_everyone()

        # åŠ è½½æ¨¡å‹
        new_model = nn.Linear(10, 5)
        new_model.load_state_dict(torch.load("/tmp/test_model.pt"))

        if accelerator.is_main_process:
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

        return True
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¿å­˜/åŠ è½½å¤±è´¥: {e}")
        return False


def test_training_loop():
    """æµ‹è¯• 7: å®Œæ•´è®­ç»ƒå¾ªç¯"""
    print("\n" + "="*80)
    print("æµ‹è¯• 7: å®Œæ•´è®­ç»ƒå¾ªç¯")
    print("="*80)

    try:
        accelerator = Accelerator()

        # åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
        model = nn.Sequential(
            nn.Linear(20, 64),
            nn.ReLU(),
            nn.Linear(64, 10)
        )
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

        model, optimizer = accelerator.prepare(model, optimizer)

        # è®­ç»ƒå¾ªç¯
        num_steps = 10
        start_time = time.time()

        for step in range(num_steps):
            # åˆ›å»ºéšæœºæ•°æ®
            x = torch.randn(8, 20, device=accelerator.device)
            y = torch.randint(0, 10, (8,), device=accelerator.device)

            # å‰å‘ä¼ æ’­
            output = model(x)
            loss = nn.functional.cross_entropy(output, y)

            # åå‘ä¼ æ’­
            accelerator.backward(loss)

            # æ¢¯åº¦è£å‰ª
            if accelerator.sync_gradients:
                accelerator.clip_grad_norm_(model.parameters(), 1.0)

            # ä¼˜åŒ–å™¨æ­¥éª¤
            optimizer.step()
            optimizer.zero_grad()

            if accelerator.is_main_process and step % 5 == 0:
                print(f"  Step {step}/{num_steps}, Loss: {loss.item():.4f}")

        elapsed = time.time() - start_time

        accelerator.wait_for_everyone()

        if accelerator.is_main_process:
            print(f"âœ… è®­ç»ƒå¾ªç¯æˆåŠŸ")
            print(f"  - æ€»æ­¥æ•°: {num_steps}")
            print(f"  - è€—æ—¶: {elapsed:.2f}s")
            print(f"  - é€Ÿåº¦: {num_steps/elapsed:.2f} steps/s")

        return True
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¾ªç¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_mixed_precision():
    """æµ‹è¯• 8: æ··åˆç²¾åº¦è®­ç»ƒ"""
    print("\n" + "="*80)
    print("æµ‹è¯• 8: æ··åˆç²¾åº¦è®­ç»ƒ")
    print("="*80)

    try:
        accelerator = Accelerator(mixed_precision="bf16")

        model = nn.Linear(100, 100)
        optimizer = torch.optim.Adam(model.parameters())

        model, optimizer = accelerator.prepare(model, optimizer)

        # è®­ç»ƒä¸€æ­¥
        x = torch.randn(4, 100, device=accelerator.device)
        y = torch.randn(4, 100, device=accelerator.device)

        output = model(x)
        loss = nn.functional.mse_loss(output, y)

        accelerator.backward(loss)
        optimizer.step()
        optimizer.zero_grad()

        if accelerator.is_main_process:
            print(f"âœ… æ··åˆç²¾åº¦è®­ç»ƒæˆåŠŸ")
            print(f"  - æ··åˆç²¾åº¦ç±»å‹: {accelerator.mixed_precision}")
            print(f"  - å‚æ•° dtype: {next(model.parameters()).dtype}")

        return True
    except Exception as e:
        print(f"âŒ æ··åˆç²¾åº¦è®­ç»ƒå¤±è´¥: {e}")
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*80)
    print("Accelerate å¤šå¡è®­ç»ƒæµ‹è¯•å¥—ä»¶")
    print("="*80)

    # æ£€æŸ¥ CUDA
    print(f"\nğŸ” ç¯å¢ƒæ£€æŸ¥:")
    print(f"  - PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"  - CUDA å¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  - CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        print(f"  - GPU æ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"  - GPU {i}: {torch.cuda.get_device_name(i)}")

    # è¿è¡Œæµ‹è¯•
    tests = [
        ("åŸºç¡€åŠŸèƒ½", test_accelerate_basic),
        ("æ¨¡å‹åˆ†å¸ƒ", test_model_distribution),
        ("æ¢¯åº¦åŒæ­¥", test_gradient_sync),
        ("æ•°æ®åˆ†å¸ƒ", test_data_distribution),
        ("Gather æ“ä½œ", test_gather_operation),
        ("ä¿å­˜/åŠ è½½", test_save_load),
        ("è®­ç»ƒå¾ªç¯", test_training_loop),
        ("æ··åˆç²¾åº¦", test_mixed_precision),
    ]

    results = []
    for name, test_func in tests:
        try:
            result = test_func()
            results.append((name, result))
        except Exception as e:
            print(f"\nâŒ æµ‹è¯• '{name}' å¼‚å¸¸: {e}")
            results.append((name, False))

    # æ‰“å°æ€»ç»“
    print("\n" + "="*80)
    print("æµ‹è¯•æ€»ç»“")
    print("="*80)

    passed = sum(1 for _, result in results if result)
    total = len(results)

    for name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{status} - {name}")

    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")

    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Accelerate å¤šå¡è®­ç»ƒé…ç½®æ­£ç¡®ï¼")
        return 0
    else:
        print(f"\nâš ï¸  æœ‰ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return 1


if __name__ == "__main__":
    import sys
    sys.exit(main())
