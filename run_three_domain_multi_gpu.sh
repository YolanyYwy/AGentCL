#!/bin/bash
# ä¸‰åŸŸæŒç»­å­¦ä¹ å¤š GPU å¹¶è¡Œè®­ç»ƒè„šæœ¬
# åœ¨å¤šå¼  GPU ä¸ŠåŒæ—¶è¿è¡Œä¸åŒè¶…å‚æ•°é…ç½®çš„å®éªŒ

MODEL="Qwen/Qwen3-4B"
AIRLINE_TASKS="data/tau2/domains/airline/tasks.json"
RETAIL_TASKS="data/tau2/domains/retail/tasks.json"
TELECOM_TASKS="data/tau2/domains/telecom/tasks_hard_300.json"
OUTPUT_BASE="./three_domain_results"
TASKS_PER_DOMAIN=100

# åˆ›å»ºæ—¥å¿—ç›®å½•
mkdir -p logs
mkdir -p $OUTPUT_BASE

echo "=========================================="
echo "ä¸‰åŸŸæŒç»­å­¦ä¹ å¤š GPU å¹¶è¡Œè®­ç»ƒ"
echo "=========================================="
echo "è®­ç»ƒé¡ºåº: Airline â†’ Retail â†’ Telecom"
echo "æ¨¡å‹: $MODEL"
echo "æ¯åŸŸä»»åŠ¡æ•°: $TASKS_PER_DOMAIN"
echo ""

# GPU 0: Baseline (lr=1e-6, beta=0.1, group=4)
echo "ğŸš€ GPU 0: Baseline é…ç½®"
CUDA_VISIBLE_DEVICES=0 python run_three_domain_continual_learning.py \
    --airline-tasks $AIRLINE_TASKS \
    --retail-tasks $RETAIL_TASKS \
    --telecom-tasks $TELECOM_TASKS \
    --model $MODEL \
    --device cuda \
    --use-grpo \
    --learning-rate 1e-6 \
    --beta 0.1 \
    --group-size 4 \
    --tasks-per-domain $TASKS_PER_DOMAIN \
    --output-dir ${OUTPUT_BASE}/gpu0_baseline \
    > logs/gpu0_baseline.log 2>&1 &

# GPU 1: æ›´å°å­¦ä¹ ç‡ (lr=5e-7)
echo "ğŸš€ GPU 1: æ›´å°å­¦ä¹ ç‡ (lr=5e-7)"
CUDA_VISIBLE_DEVICES=1 python run_three_domain_continual_learning.py \
    --airline-tasks $AIRLINE_TASKS \
    --retail-tasks $RETAIL_TASKS \
    --telecom-tasks $TELECOM_TASKS \
    --model $MODEL \
    --device cuda \
    --use-grpo \
    --learning-rate 5e-7 \
    --beta 0.1 \
    --group-size 4 \
    --tasks-per-domain $TASKS_PER_DOMAIN \
    --output-dir ${OUTPUT_BASE}/gpu1_lr5e7 \
    > logs/gpu1_lr5e7.log 2>&1 &

# GPU 2: æ›´å¤§å­¦ä¹ ç‡ (lr=2e-6)
echo "ğŸš€ GPU 2: æ›´å¤§å­¦ä¹ ç‡ (lr=2e-6)"
CUDA_VISIBLE_DEVICES=2 python run_three_domain_continual_learning.py \
    --airline-tasks $AIRLINE_TASKS \
    --retail-tasks $RETAIL_TASKS \
    --telecom-tasks $TELECOM_TASKS \
    --model $MODEL \
    --device cuda \
    --use-grpo \
    --learning-rate 2e-6 \
    --beta 0.1 \
    --group-size 4 \
    --tasks-per-domain $TASKS_PER_DOMAIN \
    --output-dir ${OUTPUT_BASE}/gpu2_lr2e6 \
    > logs/gpu2_lr2e6.log 2>&1 &

# GPU 3: æ›´å° KL æƒ©ç½š (beta=0.05)
echo "ğŸš€ GPU 3: æ›´å° KL æƒ©ç½š (beta=0.05)"
CUDA_VISIBLE_DEVICES=3 python run_three_domain_continual_learning.py \
    --airline-tasks $AIRLINE_TASKS \
    --retail-tasks $RETAIL_TASKS \
    --telecom-tasks $TELECOM_TASKS \
    --model $MODEL \
    --device cuda \
    --use-grpo \
    --learning-rate 1e-6 \
    --beta 0.05 \
    --group-size 4 \
    --tasks-per-domain $TASKS_PER_DOMAIN \
    --output-dir ${OUTPUT_BASE}/gpu3_beta0.05 \
    > logs/gpu3_beta0.05.log 2>&1 &

# GPU 4: æ›´å¤§ KL æƒ©ç½š (beta=0.2)
echo "ğŸš€ GPU 4: æ›´å¤§ KL æƒ©ç½š (beta=0.2)"
CUDA_VISIBLE_DEVICES=4 python run_three_domain_continual_learning.py \
    --airline-tasks $AIRLINE_TASKS \
    --retail-tasks $RETAIL_TASKS \
    --telecom-tasks $TELECOM_TASKS \
    --model $MODEL \
    --device cuda \
    --use-grpo \
    --learning-rate 1e-6 \
    --beta 0.2 \
    --group-size 4 \
    --tasks-per-domain $TASKS_PER_DOMAIN \
    --output-dir ${OUTPUT_BASE}/gpu4_beta0.2 \
    > logs/gpu4_beta0.2.log 2>&1 &

# GPU 5: æ›´å¤§ group size (group=8)
echo "ğŸš€ GPU 5: æ›´å¤§ group size (group=8)"
CUDA_VISIBLE_DEVICES=5 python run_three_domain_continual_learning.py \
    --airline-tasks $AIRLINE_TASKS \
    --retail-tasks $RETAIL_TASKS \
    --telecom-tasks $TELECOM_TASKS \
    --model $MODEL \
    --device cuda \
    --use-grpo \
    --learning-rate 1e-6 \
    --beta 0.1 \
    --group-size 8 \
    --tasks-per-domain $TASKS_PER_DOMAIN \
    --output-dir ${OUTPUT_BASE}/gpu5_group8 \
    > logs/gpu5_group8.log 2>&1 &

# GPU 6: æ›´å° group size (group=2)
echo "ğŸš€ GPU 6: æ›´å° group size (group=2)"
CUDA_VISIBLE_DEVICES=6 python run_three_domain_continual_learning.py \
    --airline-tasks $AIRLINE_TASKS \
    --retail-tasks $RETAIL_TASKS \
    --telecom-tasks $TELECOM_TASKS \
    --model $MODEL \
    --device cuda \
    --use-grpo \
    --learning-rate 1e-6 \
    --beta 0.1 \
    --group-size 2 \
    --tasks-per-domain $TASKS_PER_DOMAIN \
    --output-dir ${OUTPUT_BASE}/gpu6_group2 \
    > logs/gpu6_group2.log 2>&1 &

# GPU 7: ä¸ä½¿ç”¨ GRPO (ä»…è¯„ä¼°)
echo "ğŸš€ GPU 7: ä¸ä½¿ç”¨ GRPO (ä»…è¯„ä¼°)"
CUDA_VISIBLE_DEVICES=7 python run_three_domain_continual_learning.py \
    --airline-tasks $AIRLINE_TASKS \
    --retail-tasks $RETAIL_TASKS \
    --telecom-tasks $TELECOM_TASKS \
    --model $MODEL \
    --device cuda \
    --no-grpo \
    --tasks-per-domain $TASKS_PER_DOMAIN \
    --output-dir ${OUTPUT_BASE}/gpu7_no_grpo \
    > logs/gpu7_no_grpo.log 2>&1 &

echo ""
echo "=========================================="
echo "âœ… æ‰€æœ‰ 8 ä¸ªè®­ç»ƒä»»åŠ¡å·²å¯åŠ¨ï¼"
echo "=========================================="
echo ""
echo "å®éªŒé…ç½®:"
echo "  GPU 0: Baseline (lr=1e-6, beta=0.1, group=4)"
echo "  GPU 1: æ›´å°å­¦ä¹ ç‡ (lr=5e-7)"
echo "  GPU 2: æ›´å¤§å­¦ä¹ ç‡ (lr=2e-6)"
echo "  GPU 3: æ›´å° KL æƒ©ç½š (beta=0.05)"
echo "  GPU 4: æ›´å¤§ KL æƒ©ç½š (beta=0.2)"
echo "  GPU 5: æ›´å¤§ group size (group=8)"
echo "  GPU 6: æ›´å° group size (group=2)"
echo "  GPU 7: ä¸ä½¿ç”¨ GRPO (ä»…è¯„ä¼°)"
echo ""
echo "ç›‘æ§å‘½ä»¤:"
echo "  æŸ¥çœ‹æ‰€æœ‰æ—¥å¿—: tail -f logs/gpu*.log"
echo "  æŸ¥çœ‹ç‰¹å®š GPU: tail -f logs/gpu0_baseline.log"
echo "  æŸ¥çœ‹ GPU ä½¿ç”¨: watch -n 1 nvidia-smi"
echo "  æŸ¥çœ‹è¿›ç¨‹: ps aux | grep python"
echo ""
echo "åœæ­¢æ‰€æœ‰è®­ç»ƒ: pkill -f run_three_domain_continual_learning.py"
echo "=========================================="

# ç­‰å¾…æ‰€æœ‰åå°ä»»åŠ¡å®Œæˆ
wait

echo ""
echo "=========================================="
echo "ğŸ‰ æ‰€æœ‰è®­ç»ƒä»»åŠ¡å·²å®Œæˆï¼"
echo "=========================================="
echo "ç»“æœä¿å­˜åœ¨: $OUTPUT_BASE"
echo ""
echo "æŸ¥çœ‹ç»“æœ:"
echo "  cat ${OUTPUT_BASE}/gpu0_baseline/metrics.json"
echo "  python analyze_three_domain_results.py --results-dir $OUTPUT_BASE"
